{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0320edb",
   "metadata": {},
   "source": [
    "\n",
    "### 트랜스포머 모델의 학습 및 최적화\n",
    "\n",
    "트랜스포머 모델의 학습 및 최적화 과정은 모델의 성능을 극대화하기 위해 여러 기법을 사용하는 복잡한 과정입니다. 주요 기법으로는 Adam Optimizer, 학습률 스케쥴링, 그리고 레이블 스무딩이 있습니다. 각 기법이 모델 학습에 어떻게 기여하는지 살펴보겠습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3248073d",
   "metadata": {},
   "source": [
    "\n",
    "### Adam Optimizer\n",
    "\n",
    "Adam(Adaptive Moment Estimation) Optimizer는 확률적 경사 하강법(SGD) 기반의 최적화 알고리즘입니다. 이는 1차 모멘트(평균)와 2차 모멘트(분산)를 사용하여 학습률을 조정합니다. Adam Optimizer는 다음과 같은 수식으로 정의됩니다:\n",
    "\n",
    "1. **1차 및 2차 모멘트 계산**:\n",
    "   $$\n",
    "   m_t = \\beta_1 m_{t-1} + (1 - \\beta_1)g_t\n",
    "   $$\n",
    "   $$\n",
    "   v_t = \\beta_2 v_{t-1} + (1 - \\beta_2)g_t^2\n",
    "   $$\n",
    "\n",
    "2. **바이어스 보정**:\n",
    "   $$\n",
    "   \\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}\n",
    "   $$\n",
    "   $$\n",
    "   \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}\n",
    "   $$\n",
    "\n",
    "3. **파라미터 업데이트**:\n",
    "   $$\n",
    "   \\theta_t = \\theta_{t-1} - \\alpha \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}\n",
    "   $$\n",
    "\n",
    "Adam Optimizer는 학습률을 개별 파라미터마다 동적으로 조정하여 빠른 수렴과 안정적인 학습을 제공합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402db1b0",
   "metadata": {},
   "source": [
    "\n",
    "### 학습률 스케쥴링\n",
    "\n",
    "학습률 스케쥴링은 학습 과정 중에 학습률을 동적으로 조정하는 방법입니다. 일반적으로 학습 초반에는 큰 학습률을 사용하여 빠르게 최적값에 접근하고, 후반에는 작은 학습률을 사용하여 미세 조정합니다. 대표적인 학습률 스케쥴링 기법으로는 다음과 같은 것들이 있습니다:\n",
    "\n",
    "1. **Step Decay**:\n",
    "   - 일정 에폭(epoch)마다 학습률을 감소시키는 방법입니다.\n",
    "   $$\n",
    "   \\alpha_t = \\alpha_0 \\cdot \\gamma^{\\lfloor \\frac{t}{T} \\rfloor}\n",
    "   $$\n",
    "\n",
    "2. **Exponential Decay**:\n",
    "   - 학습률을 지수 함수 형태로 감소시키는 방법입니다.\n",
    "   $$\n",
    "   \\alpha_t = \\alpha_0 \\cdot e^{-kt}\n",
    "   $$\n",
    "\n",
    "3. **Cosine Annealing**:\n",
    "   - 학습률을 코사인 함수 형태로 조정하여 점진적으로 감소시키는 방법입니다.\n",
    "   $$\n",
    "   \\alpha_t = \\alpha_{min} + \\frac{1}{2}(\\alpha_{max} - \\alpha_{min})(1 + \\cos(\\frac{t}{T}\\pi))\n",
    "   $$\n",
    "\n",
    "학습률 스케쥴링은 과적합을 방지하고, 더 나은 수렴을 유도할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e73846",
   "metadata": {},
   "source": [
    "\n",
    "### 레이블 스무딩 (Label Smoothing)\n",
    "\n",
    "레이블 스무딩은 모델이 과도하게 확신(confident)하는 것을 방지하기 위해 정답 레이블을 약간 부드럽게 만드는 기법입니다. 이는 일반적으로 원-핫 인코딩된 레이블을 약간 수정하여 사용됩니다. 레이블 스무딩을 적용하면 다음과 같이 레이블이 변합니다:\n",
    "\n",
    "원-핫 인코딩 레이블:\n",
    "$$\n",
    "[0, 0, 1, 0] \\rightarrow [0, 0, 0.9, 0]\n",
    "$$\n",
    "\n",
    "레이블 스무딩 적용 후:\n",
    "$$\n",
    "[0, 0, 1, 0] \\rightarrow [0.05, 0.05, 0.85, 0.05]\n",
    "$$\n",
    "\n",
    "이렇게 하면 모델이 지나치게 확신하지 않도록 하여, 일반화 성능을 향상시킬 수 있습니다.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
