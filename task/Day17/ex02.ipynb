{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc5aa4ae",
   "metadata": {},
   "source": [
    "\n",
    "### Attention 메커니즘이 필요한 이유\n",
    "\n",
    "Attention 메커니즘은 시퀀스 처리 모델(예: RNN, LSTM)에서의 한계를 극복하기 위해 개발되었습니다. 기존 시퀀스 처리 모델은 긴 문장을 처리할 때, 특히 멀리 떨어진 단어들 간의 관계를 학습하는 데 어려움을 겪습니다. Attention 메커니즘은 다음과 같은 이유로 필요합니다:\n",
    "\n",
    "1. **긴 문장 처리**: RNN과 LSTM은 긴 문장을 처리할 때 과거 정보를 잃어버릴 수 있습니다. Attention은 각 단어에 가중치를 부여하여 전체 문장 정보를 고려할 수 있게 합니다.\n",
    "2. **병렬 처리**: RNN과 LSTM은 순차적으로 처리해야 하지만, Attention 메커니즘은 병렬 처리가 가능합니다. 이는 연산 속도를 크게 향상시킵니다.\n",
    "3. **효율적인 관계 학습**: 문장의 각 단어가 다른 단어와의 관계를 학습할 수 있게 하여, 문맥을 더 잘 이해할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d8af1d",
   "metadata": {},
   "source": [
    "\n",
    "### Scaled Dot-Product Attention\n",
    "\n",
    "Scaled Dot-Product Attention은 세 가지 입력값으로 구성됩니다: Queries (Q), Keys (K), 그리고 Values (V). 이 메커니즘은 다음과 같이 작동합니다:\n",
    "\n",
    "1. **점곱 연산**: Query와 Key 간의 점곱(dot product)을 계산하여 유사도를 측정합니다.\n",
    "2. **스케일링**: 점곱 값을 Key의 차원(D_k)의 제곱근으로 나누어 스케일링합니다. 이는 큰 값으로 인한 그래디언트 소실 문제를 완화합니다.\n",
    "3. **소프트맥스**: 스케일된 값을 소프트맥스 함수를 통해 확률 분포로 변환합니다.\n",
    "4. **가중합**: 이 확률 값을 사용하여 Value의 가중합을 계산합니다.\n",
    "\n",
    "수식:\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{D_k}}\\right)V\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9074e1d0",
   "metadata": {},
   "source": [
    "\n",
    "### Multi-Head Attention\n",
    "\n",
    "Multi-Head Attention은 여러 개의 Scaled Dot-Product Attention을 병렬로 수행하여 서로 다른 표현 공간에서의 관계를 학습합니다. 이는 모델이 다양한 문맥을 고려할 수 있게 합니다.\n",
    "\n",
    "1. **입력 분할**: Query, Key, Value를 여러 헤드로 분할합니다.\n",
    "2. **병렬 처리**: 각 헤드에서 Scaled Dot-Product Attention을 병렬로 계산합니다.\n",
    "3. **결합**: 각 헤드의 출력을 결합(concatenate)합니다.\n",
    "4. **선형 변환**: 결합된 출력을 선형 변환하여 최종 출력을 얻습니다.\n",
    "\n",
    "수식:\n",
    "$$\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\text{head}_2, \\ldots, \\text{head}_h)W^O\n",
    "$$\n",
    "여기서 각 헤드는 다음과 같이 계산됩니다:\n",
    "$$\n",
    "\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n",
    "$$\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
