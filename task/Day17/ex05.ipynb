{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c910b9bb",
   "metadata": {},
   "source": [
    "\n",
    "### LLM 모델의 평가 방법\n",
    "\n",
    "대규모 언어 모델(LLM)을 평가하는 것은 모델의 성능을 측정하고 비교하는 데 중요한 단계입니다. 다양한 평가 방법들이 있으며, 이 중 BLEU 점수는 대표적인 방법 중 하나입니다. BLEU 점수 외에도 여러 가지 평가 방법들이 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377d5594",
   "metadata": {},
   "source": [
    "\n",
    "### BLEU 점수 (BLEU Scores)\n",
    "\n",
    "BLEU(Bilingual Evaluation Understudy) 점수는 기계 번역의 품질을 평가하기 위해 개발된 방법으로, 생성된 텍스트와 참조(reference) 텍스트 간의 유사성을 측정합니다. BLEU 점수는 n-그램 매칭을 기반으로 하며, 0에서 1 사이의 값을 가집니다. 1에 가까울수록 참조 텍스트와 유사하다는 것을 의미합니다.\n",
    "\n",
    "BLEU 점수 계산 방식은 다음과 같습니다:\n",
    "\n",
    "1. **n-그램 매칭**: 생성된 텍스트와 참조 텍스트 간의 n-그램을 비교합니다. n-그램은 연속된 n개의 단어 시퀀스를 의미합니다.\n",
    "2. **정확도(Precision)**: 생성된 텍스트의 n-그램 중 참조 텍스트에 존재하는 n-그램의 비율을 계산합니다.\n",
    "3. **길이 패널티**: 생성된 텍스트의 길이가 참조 텍스트와 크게 차이날 경우 패널티를 적용합니다.\n",
    "\n",
    "BLEU 점수는 주로 기계 번역, 텍스트 생성 등의 작업에서 많이 사용됩니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921cda61",
   "metadata": {},
   "source": [
    "\n",
    "### 다른 평가 방법들\n",
    "\n",
    "BLEU 점수 외에도 LLM 모델을 평가하는 여러 방법들이 있습니다. 각각의 방법은 모델의 성능을 다양한 관점에서 평가할 수 있게 해줍니다.\n",
    "\n",
    "1. **ROUGE 점수 (ROUGE Scores)**:\n",
    "   - ROUGE(Recall-Oriented Understudy for Gisting Evaluation)는 주로 텍스트 요약의 품질을 평가하기 위해 사용됩니다. ROUGE-N, ROUGE-L, ROUGE-W 등의 변형이 있으며, 주로 n-그램 매칭, 장문 공통 부분(LCS)을 기반으로 평가합니다.\n",
    "\n",
    "2. **METEOR 점수 (METEOR Scores)**:\n",
    "   - METEOR(Metric for Evaluation of Translation with Explicit ORdering)는 BLEU 점수의 단점을 보완하기 위해 개발된 방법입니다. 어휘 변형, 어순 일치 등을 고려하여 평가합니다. 주로 기계 번역 평가에 사용됩니다.\n",
    "\n",
    "3. **CIDEr 점수 (CIDEr Scores)**:\n",
    "   - CIDEr(Consensus-based Image Description Evaluation)는 이미지 캡셔닝 작업의 품질을 평가하기 위해 사용됩니다. 참조 캡션과 생성된 캡션 간의 TF-IDF 가중치를 계산하여 유사성을 측정합니다.\n",
    "\n",
    "4. **PPL (Perplexity)**:\n",
    "   - 혼란도(Perplexity)는 언어 모델의 예측 성능을 평가하는 지표로, 모델이 주어진 텍스트의 확률을 얼마나 잘 예측하는지를 나타냅니다. 값이 낮을수록 모델의 성능이 좋음을 의미합니다.\n",
    "\n",
    "5. **휴먼 평가 (Human Evaluation)**:\n",
    "   - 자동화된 지표 외에도, 실제 사용자나 전문가가 생성된 텍스트의 품질을 평가하는 휴먼 평가가 있습니다. 이는 모델의 자연스러움, 유용성, 정확성 등을 종합적으로 평가할 수 있습니다.\n",
    "\n",
    "각 평가 방법은 모델의 다른 측면을 평가할 수 있으며, 특정 작업에 적합한 평가 방법을 선택하는 것이 중요합니다.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
