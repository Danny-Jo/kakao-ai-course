{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aaa6ce32",
   "metadata": {},
   "source": [
    "\n",
    "### Transformer 모델의 인코더와 디코더 구조 설명\n",
    "\n",
    "이 노트북은 Transformer 모델의 인코더와 디코더 구조에 대해 설명하고, 이를 시각화한 도식을 포함합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d9a3b8",
   "metadata": {},
   "source": [
    "\n",
    "#### 인코더 (Encoder)\n",
    "1. **Input Embedding**: 입력 문장을 고정된 차원의 벡터로 변환.\n",
    "2. **Positional Encoding**: 입력 벡터에 위치 정보를 추가하여 순서 정보를 보존.\n",
    "3. **Multi-Head Attention (Self)**: 각 단어가 문장의 다른 모든 단어와 관계를 학습.\n",
    "4. **Add & Norm**: Attention 출력과 입력을 더하고, 정규화.\n",
    "5. **Feed Forward**: 각 위치에서 독립적으로 적용되는 완전 연결 층.\n",
    "6. **Add & Norm**: Feed Forward 출력과 입력을 더하고, 정규화.\n",
    "7. **N x (Stack of Layers)**: 위 과정을 N번 반복.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76217886",
   "metadata": {},
   "source": [
    "\n",
    "#### 디코더 (Decoder)\n",
    "1. **Input Embedding**: 출력 문장을 고정된 차원의 벡터로 변환.\n",
    "2. **Positional Encoding**: 출력 벡터에 위치 정보를 추가.\n",
    "3. **Multi-Head Attention (Self)**: 각 단어가 출력 문장의 다른 모든 단어와 관계를 학습.\n",
    "4. **Add & Norm**: Attention 출력과 입력을 더하고, 정규화.\n",
    "5. **Multi-Head Attention (Encoder-Decoder)**: 인코더의 출력과 디코더의 입력 간의 관계를 학습.\n",
    "6. **Add & Norm**: Encoder-Decoder Attention 출력과 입력을 더하고, 정규화.\n",
    "7. **Feed Forward**: 각 위치에서 독립적으로 적용되는 완전 연결 층.\n",
    "8. **Add & Norm**: Feed Forward 출력과 입력을 더하고, 정규화.\n",
    "9. **N x (Stack of Layers)**: 위 과정을 N번 반복.\n",
    "10. **Linear & Softmax**: 최종 출력층으로, 다음 단어의 확률을 계산.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e0c902",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "# Define the Transformer Encoder-Decoder structure\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Encoder layers\n",
    "encoder_layers = ['Input Embedding', 'Positional Encoding', 'Multi-Head Attention (Self)', 'Add & Norm', \n",
    "                  'Feed Forward', 'Add & Norm', 'N x (Stack of Layers)']\n",
    "\n",
    "# Decoder layers\n",
    "decoder_layers = ['Input Embedding', 'Positional Encoding', 'Multi-Head Attention (Self)', 'Add & Norm', \n",
    "                  'Multi-Head Attention (Encoder-Decoder)', 'Add & Norm', 'Feed Forward', 'Add & Norm', \n",
    "                  'N x (Stack of Layers)', 'Linear & Softmax']\n",
    "\n",
    "# Add encoder nodes\n",
    "for i, layer in enumerate(encoder_layers):\n",
    "    G.add_node(f'Encoder {i+1}\\n{layer}')\n",
    "\n",
    "# Add decoder nodes\n",
    "for i, layer in enumerate(decoder_layers):\n",
    "    G.add_node(f'Decoder {i+1}\\n{layer}')\n",
    "\n",
    "# Add connections between encoder layers\n",
    "for i in range(len(encoder_layers) - 1):\n",
    "    G.add_edge(f'Encoder {i+1}\\n{encoder_layers[i]}', f'Encoder {i+2}\\n{encoder_layers[i+1]}')\n",
    "\n",
    "# Add connections between decoder layers\n",
    "for i in range(len(decoder_layers) - 1):\n",
    "    G.add_edge(f'Decoder {i+1}\\n{decoder_layers[i]}', f'Decoder {i+2}\\n{decoder_layers[i+1]}')\n",
    "\n",
    "# Add connection between encoder and decoder (Multi-Head Attention)\n",
    "G.add_edge(f'Encoder 6\\nAdd & Norm', f'Decoder 5\\nMulti-Head Attention (Encoder-Decoder)')\n",
    "\n",
    "# Draw the graph\n",
    "plt.figure(figsize=(16, 12))\n",
    "pos = nx.spring_layout(G, seed=42)\n",
    "nx.draw(G, pos, with_labels=True, node_size=5000, node_color='lightblue', font_size=10, font_weight='bold', arrows=True)\n",
    "plt.title('Transformer Encoder-Decoder Structure')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
